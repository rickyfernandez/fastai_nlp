{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeqtoSeq using attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Config().data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ricky/.fastai/data/giga-fren.tgz.1'),\n",
       " PosixPath('/home/ricky/.fastai/data/oxford-iiit-pet'),\n",
       " PosixPath('/home/ricky/.fastai/data/mnist_png'),\n",
       " PosixPath('/home/ricky/.fastai/data/imdb'),\n",
       " PosixPath('/home/ricky/.fastai/data/mnist_png.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/imdb_sample.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren'),\n",
       " PosixPath('/home/ricky/.fastai/data/imdb_sample'),\n",
       " PosixPath('/home/ricky/.fastai/data/oxford-iiit-pet.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/human_numbers.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/imdb.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/data/human_numbers')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Config().data_path()/'giga-fren'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ricky/.fastai/data/giga-fren/models'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/cc.en.300.bin'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/questions_easy.csv'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/cc.fr.300.bin'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/data_save.pkl'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/giga-fren.release2.fixed.en'),\n",
       " PosixPath('/home/ricky/.fastai/data/giga-fren/giga-fren.release2.fixed.fr')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routines that already implemented to create model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n",
    "    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n",
    "    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
    "        else:         \n",
    "            res_x[i,:len(s[0])],res_y[i,:len(s[1])] = LongTensor(s[0]),LongTensor(s[1])\n",
    "    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n",
    "    return res_x,res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n",
    "               dl_tfms=None, pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ricky/.fastai/models/en_emb.pth'),\n",
       " PosixPath('/home/ricky/.fastai/models/wt103-fwd.tgz'),\n",
       " PosixPath('/home/ricky/.fastai/models/wt103-fwd'),\n",
       " PosixPath('/home/ricky/.fastai/models/fr_emb.pth')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = Config().model_path()\n",
    "model_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc = torch.load(model_path/'fr_emb.pth')\n",
    "emb_dec = torch.load(model_path/'en_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(out, targ, pad_idx=1):\n",
    "    bs,targ_len = targ.size()\n",
    "    _,out_len,vs = out.size()\n",
    "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "    return CrossEntropyFlat()(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_acc(out, targ, pad_idx=1):\n",
    "    bs,targ_len = targ.size()\n",
    "    _,out_len,vs = out.size()\n",
    "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "    out = out.argmax(2)\n",
    "    return (out==targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram():\n",
    "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "    def __eq__(self, other):\n",
    "        if len(self.ngram) != len(other.ngram): return False\n",
    "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grams(x, n, max_n=5000):\n",
    "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
    "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
    "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(learn, ds_type=DatasetType.Valid):\n",
    "    learn.model.eval()\n",
    "    inputs, targets, outputs = [],[],[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in progress_bar(learn.dl(ds_type)):\n",
    "            out = learn.model(xb)\n",
    "            for x,y,z in zip(xb,yb,out):\n",
    "                inputs.append(learn.data.train_ds.x.reconstruct(x))\n",
    "                targets.append(learn.data.train_ds.y.reconstruct(y))\n",
    "                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))\n",
    "    return inputs, targets, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusBLEU(Callback):\n",
    "    def __init__(self, vocab_sz):\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.name = 'bleu'\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        last_output = last_output.argmax(dim=-1)\n",
    "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "            self.pred_len += len(pred)\n",
    "            self.targ_len += len(targ)\n",
    "            for i in range(4):\n",
    "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                self.corrects[i] += c\n",
    "                self.counts[i]   += t\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n",
    "        return add_metrics(last_metrics, bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherForcing(LearnerCallback):\n",
    "    def __init__(self, learn, end_epoch):\n",
    "        super().__init__(learn)\n",
    "        self.end_epoch = end_epoch\n",
    "    \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        if train: return {'last_input': [last_input, last_target]}\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, **kwargs):\n",
    "        self.learn.model.pr_force = 1 - epoch/self.end_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_attn(nn.Module):\n",
    "    def __init__(self, emb_enc, emb_dec, nh, out_sl, nl=2, bos_idx=0, pad_idx=1):\n",
    "        super().__init__()\n",
    "        self.nl,self.nh,self.out_sl,self.pr_force = nl,nh,out_sl,1\n",
    "        self.bos_idx,self.pad_idx = bos_idx,pad_idx\n",
    "        self.emb_enc,self.emb_dec = emb_enc,emb_dec\n",
    "        self.emb_sz_enc,self.emb_sz_dec = emb_enc.embedding_dim,emb_dec.embedding_dim\n",
    "        self.voc_sz_dec = emb_dec.num_embeddings\n",
    "                 \n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.gru_enc = nn.GRU(self.emb_sz_enc, nh, num_layers=nl, dropout=0.25, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(2*nh, self.emb_sz_dec, bias=False)\n",
    "        \n",
    "        self.gru_dec = nn.GRU(self.emb_sz_dec + 2*nh, self.emb_sz_dec, num_layers=nl,\n",
    "                              dropout=0.1, batch_first=True)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(self.emb_sz_dec, self.voc_sz_dec)\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "        self.enc_att = nn.Linear(2*nh, self.emb_sz_dec, bias=False)\n",
    "        self.hid_att = nn.Linear(self.emb_sz_dec, self.emb_sz_dec)\n",
    "        self.V =  self.init_param(self.emb_sz_dec)\n",
    "        \n",
    "    def encoder(self, bs, inp):\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        #enc_out, hid = self.gru_enc(emb, 2*h) # think should remove 2\n",
    "        enc_out, hid = self.gru_enc(emb, h) # think should remove 2\n",
    "        \n",
    "        #pre_hid = hid.view(2, self.nl, bs, self.nh).permute(1,2,0,3).contiguous()\n",
    "        pre_hid = hid.view(2, self.nl, bs, self.nh).permute(0,2,1,3).contiguous()\n",
    "        pre_hid = pre_hid.view(self.nl, bs, 2*self.nh)\n",
    "        hid = self.out_enc(pre_hid)\n",
    "        return hid,enc_out\n",
    "    \n",
    "    def decoder(self, dec_inp, hid, enc_att, enc_out):\n",
    "        hid_att = self.hid_att(hid[-1])\n",
    "        # we have put enc_out and hid through linear layers\n",
    "        u = torch.tanh(enc_att + hid_att[:,None])\n",
    "        # we want to learn the importance of each time step\n",
    "        attn_wgts = F.softmax(u @ self.V, 1)\n",
    "        # weighted average of enc_out (which is the output at every time step)\n",
    "        ctx = (attn_wgts[...,None] * enc_out).sum(1)\n",
    "        emb = self.emb_dec(dec_inp)\n",
    "        # concatenate decoder embedding with context (we could have just\n",
    "        # used the hidden state that came out of the decoder, if we weren't\n",
    "        # using attention)\n",
    "        outp, hid = self.gru_dec(torch.cat([emb, ctx], 1)[:,None], hid)\n",
    "        outp = self.out(self.out_drop(outp[:,0]))\n",
    "        return hid, outp\n",
    "        \n",
    "    def show(self, nm,v):\n",
    "        if False: print(f\"{nm}={v[nm].shape}\")\n",
    "        \n",
    "    def forward(self, inp, targ=None):\n",
    "        bs, sl = inp.size()\n",
    "        hid,enc_out = self.encoder(bs, inp)\n",
    "#        self.show(\"hid\",vars())\n",
    "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
    "        enc_att = self.enc_att(enc_out)\n",
    "        \n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            hid, outp = self.decoder(dec_inp, hid, enc_att, enc_out)\n",
    "            res.append(outp)\n",
    "            dec_inp = outp.max(1)[1]\n",
    "            if (dec_inp==self.pad_idx).all(): break\n",
    "            if (targ is not None) and (random.random()<self.pr_force):\n",
    "                if i>=targ.shape[1]: continue\n",
    "                dec_inp = targ[:,i]\n",
    "        return torch.stack(res, dim=1)\n",
    "\n",
    "    def initHidden(self, bs): return one_param(self).new_zeros(2*self.nl, bs, self.nh)\n",
    "    def init_param(self, *sz): return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc = torch.load(model_path/'fr_emb.pth')\n",
    "emb_dec = torch.load(model_path/'en_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Seq2SeqRNN_attn(emb_enc, emb_dec, 256, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking step by step into the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, sl = xb.size()\n",
    "bs, sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, sl = xb.size()\n",
    "h = m.initHidden(bs)\n",
    "h.size()   # 2 states per layer, bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 300])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, sl = xb.size()\n",
    "h = m.initHidden(bs)\n",
    "h.size()   # 2 states per layer, bi-directional  \n",
    "emb_dec = m.emb_enc(xb.cpu())\n",
    "emb_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try to understand the ordering of gru. When we use batch_first=False two tensors are returned (out, h_n) and their shapes are (seq_len, batch, num_directions * hidden_size) and (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 64, 512]), torch.Size([4, 64, 256]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = nn.GRU(300, 256, num_layers=2, dropout=0.25, batch_first=False, bidirectional=True)\n",
    "out, h_n = gru(torch.randn(30,64,300))\n",
    "out.shape, h_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1372, -0.3016, -0.0184, -0.0820, -0.1400, -0.2431, -0.3669,\n",
       "        -0.0260, -0.2836], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(30, 64, 2, 256)[0, 0, 1, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1372, -0.3016, -0.0184, -0.0820, -0.1400, -0.2431, -0.3669,\n",
       "        -0.0260, -0.2836], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.view(2, 2, 64, 256)[1, 1, 0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so this is quite hard to grasp, the words are concatenated in sequence so for out we index as first word (for bi-direcional it is the last output), first batch value, backward pass, first 10 values in hidden state.\n",
    "\n",
    "for h_n we index by layer 1, backward pass, first batch value, first 10 values in hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets find the correct ordering when batch_first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 30, 512]), torch.Size([4, 64, 256]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, sl = xb.size()\n",
    "h = m.initHidden(bs)\n",
    "h.size()   # 2 states per layer  \n",
    "emb_dec = m.emb_enc(xb.cpu())\n",
    "enc_out, hid = m.gru_enc(emb_dec, h) \n",
    "enc_out.shape, hid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1768,  0.2498,  0.0201,  0.3203,  0.0501, -0.2893,  0.0793, -0.1212,\n",
       "        -0.3139,  0.3171], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.view(64, 30, 2, 256)[0, 0, 1, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1768,  0.2498,  0.0201,  0.3203,  0.0501, -0.2893,  0.0793, -0.1212,\n",
       "        -0.3139,  0.3171], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid.view(2, 2, 64, 256)[1, 1, 0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, we see that words are consistent but can cause confusion. Compare output backward hidden state from last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(enc_out.view(64, 30, 2, 256)[0, 0, 1], hid.view(2, 2, 64, 256)[1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output of the foward output hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(enc_out.view(64, 30, 2, 256)[0, -1, 0], hid.view(2, 2, 64, 256)[1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536, 65536)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid.numel(), 2*2*64*256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now are trying to reshape the correct outputs with in the bi-direcional case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 2, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to layer, batch, direction, hidden\n",
    "#pre_hid_ = hid.view(2, 2, 64, 256).permute(1,2,0,3).contiguous()\n",
    "pre_hid_ = hid.view(2, 2, 64, 256).permute(0,2,1,3).contiguous()\n",
    "pre_hid_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output layer, first batch, forward\n",
    "np.allclose(pre_hid_[1, 0, 0], hid.view(2, 2, 64, 256)[1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 512])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now flatten bidirectional vector (layer, batch, bi-hidden)\n",
    "pre_hid_flat = pre_hid_.view(2, 64, 2*256)\n",
    "pre_hid_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_hid_flat[1, 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check lets grab the output of each directional rnn and concat compare with our flatten array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = hid.view(2, 2, 64, 256)[1, 0, 0]\n",
    "right = hid.view(2, 2, 64, 256)[1, 1, 0]\n",
    "\n",
    "np.allclose(pre_hid_flat[1,0], torch.cat([left, right], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great its in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally multiply by dense matrix to french vector for each layer\n",
    "out_enc = m.out_enc(pre_hid_flat)\n",
    "out_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking step by step into decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 300]), torch.Size([64, 30, 512]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, sl = xb.size()\n",
    "hid, enc_out = m.encoder(bs, xb.cpu())\n",
    "hid.shape, enc_out.shape # is the flatten hidden state dense to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inp = xb.cpu().new_zeros(bs).long() + m.bos_idx\n",
    "dec_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=300, bias=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.enc_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 300])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_att = m.enc_att(enc_out) # enc_out is for every batch every word bi-direction hidden state\n",
    "enc_att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets just do one step in the decorder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab output vector\n",
    "np.allclose(h[-1], h[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 300]), torch.Size([64, 1, 300]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not sure why you need another dense layer?\n",
    "hid_att = m.hid_att(hid[-1])\n",
    "hid_att.shape, hid_att[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 300])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we add hidden vector to every output state\n",
    "a = enc_att + hid_att[:,None]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform dot product to generate weights\n",
    "u = torch.tanh(enc_att + hid_att[:, None])\n",
    "(u @ m.V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 30]),\n",
       " tensor(1.0000, grad_fn=<SumBackward0>),\n",
       " torch.Size([64, 30, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts = F.softmax(u @ m.V, 1)\n",
    "attn_wgts.shape, attn_wgts[0].sum(), attn_wgts[...,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now perform the weighted avergae, multiply weight at each word then sum in the word\n",
    "# axes\n",
    "ctx = (attn_wgts[...,None] * enc_out).sum(1)\n",
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 812]), torch.Size([64, 1, 812]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = m.emb_dec(dec_inp)\n",
    "a = torch.cat([emb, ctx], 1)\n",
    "a.shape, a[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 300]), torch.Size([2, 64, 300]), torch.Size([64, 300]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp, hid = m.gru_dec(torch.cat([emb, ctx], 1)[:,None], hid)\n",
    "outp.shape, hid.shape, outp[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 300]), torch.Size([64, 8144]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp = m.out(m.out_drop(outp[:,0]))\n",
    "hid.shape, outp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we finally have the vector of probabilities for french words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 5242, 2810, 6753, 8143, 5830,    0, 3807, 5727, 5724, 6842, 3807,\n",
       "        6753, 7047, 6753, 6052,    0,    6, 6052,    0,    0, 2588,    0, 5727,\n",
       "           0, 5242, 6540,    0,    0, 4896, 5724, 6405,    0,    0,    0,    0,\n",
       "        8143, 6753, 3247, 7437, 7487, 4940, 6052, 5727, 4896,    0,    0, 5830,\n",
       "        2588, 2588,    0, 6753,    0, 3240,    0,    0,    0, 4693, 6753, 5242,\n",
       "        7437,    0, 5242, 6052])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.max(1)[1] # index of predicted word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqRNN_attn(emb_enc, emb_dec, 256, 30)\n",
    "learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],\n",
    "                callback_fns=partial(TeacherForcing, end_epoch=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>seq2seq_acc</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.561432</td>\n",
       "      <td>4.271276</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>0.253677</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.231420</td>\n",
       "      <td>3.530648</td>\n",
       "      <td>0.570698</td>\n",
       "      <td>0.334106</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.950797</td>\n",
       "      <td>2.987753</td>\n",
       "      <td>0.604998</td>\n",
       "      <td>0.341023</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.808132</td>\n",
       "      <td>3.771054</td>\n",
       "      <td>0.503009</td>\n",
       "      <td>0.399898</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.665941</td>\n",
       "      <td>3.475111</td>\n",
       "      <td>0.529311</td>\n",
       "      <td>0.405679</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.577948</td>\n",
       "      <td>3.763448</td>\n",
       "      <td>0.498438</td>\n",
       "      <td>0.425592</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.550794</td>\n",
       "      <td>3.409750</td>\n",
       "      <td>0.514386</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.565027</td>\n",
       "      <td>3.716606</td>\n",
       "      <td>0.492722</td>\n",
       "      <td>0.431281</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.376277</td>\n",
       "      <td>3.712253</td>\n",
       "      <td>0.494533</td>\n",
       "      <td>0.433335</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.328058</td>\n",
       "      <td>3.657451</td>\n",
       "      <td>0.490790</td>\n",
       "      <td>0.433772</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.281367</td>\n",
       "      <td>3.643757</td>\n",
       "      <td>0.494195</td>\n",
       "      <td>0.442317</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.287680</td>\n",
       "      <td>3.687223</td>\n",
       "      <td>0.490379</td>\n",
       "      <td>0.438856</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.163054</td>\n",
       "      <td>3.724290</td>\n",
       "      <td>0.487769</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.164978</td>\n",
       "      <td>3.731910</td>\n",
       "      <td>0.486810</td>\n",
       "      <td>0.438514</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.090921</td>\n",
       "      <td>3.712541</td>\n",
       "      <td>0.488154</td>\n",
       "      <td>0.437701</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
